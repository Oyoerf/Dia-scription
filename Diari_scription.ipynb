{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Diari-scription script !\n",
        "This python script allows you to transcribe any audio file with speech. Please prefer .mp3 or .wav file formats.\n",
        "The script also recognize who is speaking, an operation called \"diarization\".\n",
        "This script relies on Whisper, a general-purpose speech recognition model developed by [OpenAI](https://openai.com/research/whisper). The original paper (Radford et al., 2022) can be found [here](https://arxiv.org/abs/2212.04356).\n",
        "\n",
        "HuggingFace is a nice resource, and the transcription part of the code comes from [there](https://huggingface.co/openai/whisper-large-v3) !\n",
        "The diarization part of the code comes from Max Bain on GitHub and his [WhisperX](https://github.com/m-bain/whisperX) code !\n",
        "\n",
        "\n",
        "## A quick requirement\n",
        "Before anything else, please visit the HuggingFace website to [request a token](https://huggingface.co/settings/tokens). Please copy paste it in the **HuggingFaceToken** form right below, without adding any \" \" or ' ' symbol ! Don't worry, you only have to request the token once, if you don't reinitialize it.\n",
        "\n",
        "After this first step is completed, please select a **GPU** backend in Google Colab (usually in the top-right corner). This script is meant to be run on a GPU, not a CPU.\n",
        "\n",
        "\n",
        "If you want to connect your personal Google Drive account to Colab, you can do so by clicking on the folder icon on the left-side bar, and press on the folder icon that contains the Google Drive triangle, called \"Mount Drive\" *(\"Installer Drive\").\n",
        "\n",
        "## Files management\n",
        "To load you audio file, click on the folder icon on the left-side panel, and press on the first icon called \"Upload to session storage\" *(\"Aucun fichier selectionné\")*.\n",
        "\n",
        "The transcription of the audio file is exported into a .txt file. This text file will have the **same** name as the audio file you chose as input.\n",
        "\n",
        "**Be careful**, if you run this script multiple times with the same input audio file, it will overwrite any previous file with the same name.\n",
        "\n",
        "## Launching the script\n",
        "You just have to fill out the forms below and go to Runtime -> Run all *(Exécution -> Tout exécuter)* and then wait a few minutes :)"
      ],
      "metadata": {
        "id": "zei8FAd7soVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyQxmicK8UsX"
      },
      "outputs": [],
      "source": [
        "# @title Initialisation script { display-mode: \"form\" }\n",
        "\n",
        "#initialization and imports\n",
        "!pip install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia &> /dev/null\n",
        "!pip install git+https://github.com/m-bain/whisperx.git &> /dev/null\n",
        "!sudo apt update &> /dev/null\n",
        "!sudo apt install ffmpeg &> /dev/null\n",
        "\n",
        "import whisperx\n",
        "import gc\n",
        "\n",
        "#cuda is for running the inference on the Google Colab's GPU\n",
        "device = \"cuda\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"int8\" # change to \"int8\" if low on GPU mem (may reduce accuracy). Another option is \"float16\"\n",
        "\n",
        "HuggingFaceToken = \"\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the part you have to interact with when you want to transcribe an interview !\n",
        "\n",
        "You're just a few steps far from your transcript !\n",
        "1. Just select the path to your audio file (*the easiest way is that your file is hosted on your personal Google Drive account*) is the field **AudioFilePath**.\n",
        "2. The task type (transcription or translation to english) in the **TaskType** dropwodn menu.\n",
        "3. Finally, you can choose the language spoken in the interview in the **LanguageSpokenShort** field. If left blank, the model will detect by itself the most probable one ! Please use the abbreviation : fr for french, en for english, etc. A list of abbreviations can be found [here](https://www.loc.gov/standards/iso639-2/php/code_list.php).\n",
        "4. Choose the minimum and maximum number of speakers in your interview. These numbers can be equal (if you know haw many people are speaking) but MaxNumberSpeakers should **always be >= to MinNumberSpeakers** !!!\n"
      ],
      "metadata": {
        "id": "Lx600ryTVMtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fill out the form ! { display-mode: \"form\" }\n",
        "AudioFilePath = \"\" # @param {type:\"string\"}\n",
        "TaskType = \"transcribe\" # @param [\"transcribe\", \"translate\"]\n",
        "LanguageSpokenShort = \"\" # @param {type:\"string\"}\n",
        "MinNumberSpeakers = 1 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "MaxNumberSpeakers = 1 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "#the path to your audio file of the interview.\n",
        "#DON'T FORGET THE EXTENSION !\n",
        "audio_file = AudioFilePath\n",
        "task_type = TaskType #the task type can also be \"translate\", to translate IN ENGLISH\n",
        "language_spoken = LanguageSpokenShort #please use"
      ],
      "metadata": {
        "id": "5jpBdHnDORu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the transcription part, where the audio is turned into text. But the speakers are not identified yet !"
      ],
      "metadata": {
        "id": "vpK0IkBfUF4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLcAa9Hd7iaO"
      },
      "outputs": [],
      "source": [
        "# @title Transcription { display-mode: \"form\" }\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v3\", device, compute_type=compute_type, task=task_type, language=language_spoken)\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "#print(result[\"segments\"]) # before alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "#print(result1[\"segments\"]) # after alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the part where the speakers get identified and the text they pronounced is assigned to them !\n",
        "This step is called the '**diarization**'."
      ],
      "metadata": {
        "id": "VUR-Bd1mUMl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3ofxCpoCku-"
      },
      "outputs": [],
      "source": [
        "# @title Speaker identification { display-mode: \"form\" }\n",
        "# 3. Assign speaker labels\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HuggingFaceToken, device=device)\n",
        "\n",
        "# add min/max number of speakers if known\n",
        "diarize_segments = diarize_model(audio, min_speakers=MinNumberSpeakers, max_speakers=MaxNumberSpeakers)\n",
        "\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result, fill_nearest=False)\n",
        "#print(diarize_segments)\n",
        "segments = result[\"segments\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the part where it all comes together, transcription and diarization. The result is formatted and printed into a file that has the **same name** as the audio input."
      ],
      "metadata": {
        "id": "Z3Cq-TeaUXzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4WY8_3YF1UJ"
      },
      "outputs": [],
      "source": [
        "# @title Where it all comes together { display-mode: \"form\" }\n",
        "# @title\n",
        "#on reprend le nom du fichier audio et on en fait un nom de fichier au format .txt\n",
        "file_name=audio_file.split('.')[0]+'.txt'\n",
        "#print(file_name)\n",
        "\n",
        "# on ouvre un fichier .txt pour écrire la retranscription dedans\n",
        "# on l'ouvre en mode 'append' pour éviter les fausses manip\n",
        "#remplacer 'a' par 'w' ouvre en mode 'write' et réécrit tout le fichier à chaque fois\n",
        "file1 = open(file_name, \"w\", encoding='utf-8')\n",
        "#on boucle dans la liste de dictionnaires 'segments' pour séparer les changements de speaker\n",
        "file1.write(\"RETRANSCRIPTION \\n\")\n",
        "#print(segments[0]['speaker'])\n",
        "\n",
        "#variable to store the previous speaker\n",
        "speaker_prev = 'no'\n",
        "#let's run through the list of dictionnaries and extract the speaker and the text\n",
        "for i in range(len(segments)):\n",
        "    #first, we check if the 'speaker' key is present in the dictionnary\n",
        "    #sometimes, it is not, for example with ponctuation\n",
        "    if 'speaker' in segments[i]:\n",
        "        #we want to change the speaker only if it changed compared to the previous sentence\n",
        "        if segments[i]['speaker'] != speaker_prev:\n",
        "            #print('\\n', segments[i]['speaker'])\n",
        "            #file1.write(' \\n')\n",
        "            file1.write('\\n' + '\\n' + segments[i]['speaker'] + '\\n')\n",
        "            #file1.write(' \\n')\n",
        "            speaker_prev = segments[i]['speaker']\n",
        "        #print(segments[i]['text'], end=' ')\n",
        "    #whatever, we print the text, with a space at the end\n",
        "    file1.writelines(segments[i]['text'] + ' ')\n",
        "\n",
        "#Et on n'oublie pas de refermer en quittant...\n",
        "file1.close()\n",
        "print('Done ! Go check your transcription file !')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1gYPP58-E0urmY1bENYYT4u5UpZJmSh0C",
      "authorship_tag": "ABX9TyOL+bttkea1KbCzy7CgLkiU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}